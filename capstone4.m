%% Generating Music Data
clear; clc;
freq1=[130.81	138.59	146.83	155.56	164.81	174.61	185	196	207.65 ...
    220	233.08	246.94 261.6	277.2	293.7	311.1	329.6	349.2...
  370.0	392.0    415.3	440.0	466.2	493.9 523.25 554.37 587.33...
  622.25 659.25 698.46 739.99 783.99 830.61 880 932.33 987.77];

number_notes = 50;
Song_time = 10; %seconds
Fs = 8000;
prob_half = .1; %Probability that we hit a half note
prob_eighth = .1; %Probability that we hit an eight note
prob_rest = .05; %Probability that we are at a rest;    

song = randi([1 size(freq1,2)], 1, number_notes); % May not need this

%note_value = 0:.000125:Note_duration; %Change .000125 for sampling freq
%note_value_half = 0:.000125:Note_duration*2;
%note_value_eight = 0:.000125:Note_duration/2;

TEMPVALS = 40:5:240;

count = 1;
true_temp = [];
%for tempo = 60:2:140
for i = 1:30
    i
    song = randi([1 size(freq1,2)], 1, number_notes); % May not need this

    for tempo = TEMPVALS;
%        tempo

        true_temp(count) = tempo;

        Note_duration = 60/tempo;
        note_value = 0:.000125:Note_duration; %Change .000125 for sampling freq
        note_value_half = 0:.000125:Note_duration*2;
        note_value_eight = 0:.000125:Note_duration/2;
        x_val = 0:(size(note_value,2)-1);
        y = ones(size(x_val))-sin(pi.*x_val./(2*(size(note_value,2)-1)));
        x_val_half = 0:(size(note_value_half,2)-1);
        y_half = ones(size(x_val_half))-sin(pi.*x_val_half./(2*(size(note_value_half,2)-1)));
        x_val_eight = 0:(size(note_value_eight,2)-1);
        y_eight = ones(size(x_val_eight))-sin(pi.*x_val_eight./(2*(size(note_value_eight,2)-1)));
        a = [];
        Regular = true;
        for k = 1:numel(song)
            if(Regular)
                if (rand < prob_half)
                    a = [a y_half.*sin(2*pi*freq1(song(k))*note_value_half)];
                else 
                    if (rand < prob_eighth)
                        a = [a y_eight.*sin(2*pi*freq1(song(k))*note_value_eight)];
                        Regular = false;
                    else
                        if (prob_rest > rand)
                            a = [a 0*note_value];
                        else
                            a=[a y.*sin(2*pi* freq1(song(k))*note_value)];
                        end
                    end
                end

            else
                a = [a y_eight.*sin(2*pi*freq1(song(k))*note_value_eight)];
                Regular = true;
            end
        end
        timeval = 0:size(a,2)-1;
        timeval = timeval./Fs;
        time_10 = timeval(1:find(timeval == 10));
        b(count,:) = a(1:find(timeval == 10));
        fftb(count,:) = fft(b(count,:),1024);
        count = count+1;
    end
end


%

data = b';
dataFft = fftb';
p = randperm(size(data,2));
%p = 1:size(data,2);
Data = data(:,p);
DataFft = dataFft(:,p);
%Data = data;
Label = true_temp(p);
% 

%plot(time_10,b(200,:))
%xlabel('Time (s)');
%title('Time Domain Representation of sample synthetic data');
%nnstart
%sound(b(201,:))


%%
clear; clc;
% Solve an Input-Output Fitting problem with a Neural Network
% Script generated by Neural Fitting app
% Created 17-Apr-2017 22:10:38
%
% This script assumes these variables are defined:
%
%   Data - input data.
%   Label - target data.

temp1 = load('SongData1.mat');
temp2 = load('LabelData1.mat');
temp3 = load('SongFft1.mat');
x = temp1.Data;
%x = temp3.DataFft;
%x = abs(x);
%x = x(1:1000,:);
t = temp2.Label;

i1 = 1;

for hiddenLayerSize = 10
% Choose a Training Function
% For a list of all training functions type: help nntrain
% 'trainlm' is usually fastest.
% 'trainbr' takes longer but may be better for challenging problems.
% 'trainscg' uses less memory. Suitable in low memory situations.

trainFcn = 'trainscg';  % Scaled conjugate gradient backpropagation.

fprintf('Here1\n');
% Create a Fitting Network
% if ( i == 1)
%     hiddenLayerSize = 10;
% else
%     hiddenLayerSize = 20;
% end

    net = fitnet(hiddenLayerSize,trainFcn);

% Choose Input and Output Pre/Post-Processing Functions
% For a list of all processing functions type: help nnprocess
%net.input.processFcns = {'removeconstantrows','mapminmax'};
%net.output.processFcns = {'removeconstantrows','mapminmax'};
fprintf('Here2\n');

% Setup Division of Data for Training, Validation, Testing
% For a list of all data division functions type: help nndivide
net.divideFcn = 'dividerand';  % Divide data randomly
net.divideMode = 'sample';  % Divide up every sample
net.divideParam.trainRatio = 70/100;
net.divideParam.valRatio = 15/100;
net.divideParam.testRatio = 15/100;
fprintf('Here3\n');

% Choose a Performance Function
% For a list of all performance functions type: help nnperformance
net.performFcn = 'mse';  % Mean Squared Error
fprintf('Here4\n');

% Choose Plot Functions
% For a list of all plot functions type: help nnplot
net.plotFcns = {'plotperform','plottrainstate','ploterrhist', ...
    'plotregression', 'plotfit'};
fprintf('Here5\n');

% Train the Network
[net,tr] = train(net,x,t);
fprintf('Here6\n');

% Test the Network
y = net(x);
e = gsubtract(t,y);
performance = perform(net,t,y);
fprintf('Here7\n');

% Recalculate Training, Validation and Test Performance
trainTargets = t .* tr.trainMask{1};
valTargets = t .* tr.valMask{1};
testTargets = t .* tr.testMask{1};
trainPerformance = perform(net,trainTargets,y);
valPerformance = perform(net,valTargets,y);
testPerformance = perform(net,testTargets,y);

% View the Network
view(net);

% Plots
% Uncomment these lines to enable various plots.
%figure, plotperform(tr)
%figure, plottrainstate(tr)
%figure, ploterrhist(e)
%figure, plotregression(t,y)
%figure, plotfit(net,x,t)

% Deployment
% Change the (false) values to (true) to enable the following code blocks.
% See the help for each generation function for more information.
if (false)
    % Generate MATLAB function for neural network for application
    % deployment in MATLAB scripts or with MATLAB Compiler and Builder
    % tools, or simply to examine the calculations your trained neural
    % network performs.
    genFunction(net,'myNeuralNetworkFunction');
    y = myNeuralNetworkFunction(x);
end
if (false)
    % Generate a matrix-only MATLAB function for neural network code
    % generation with MATLAB Coder tools.
    genFunction(net,'myNeuralNetworkFunction','MatrixOnly','yes');
    y = myNeuralNetworkFunction(x);
end
if (false)
    % Generate a Simulink diagram for simulation or deployment with.
    % Simulink Coder tools.
    gensim(net);
end
clear net;
clear tr;

ErrBar(i1) = norm(e);
PerMet(i1) = performance;
i1 = i1 + 1;
end


%% 

nhn = 10:10:100;
figure;
bar(nhn,ErrBar);
title('Performance of SCG on Fourier Transform');
xlabel('Number of Hidden Neurons');
ylabel('||Est. Tempo - Real Tempo||_2^2');

%% 
clear; clc;

temp1 = load('SongData1.mat');
temp2 = load('LabelData1.mat');
temp3 = load('SongFft1.mat');
x1 = temp1.Data;

%h = [1 1 1 1 1 1 1 1 1];
h = ones(1000,1);
x2 = zeros(size(x1,1)+size(h,1)-1,size(x1,2));
x = zeros(2048,size(x1,2));
for i = 1:size(x1,2)
    x2(:,i) = conv(x1(:,i),h);
    x(:,i) = abs(fft(x2(:,i),2048));
end

% ^Shows somewhat improvment...

%% Also works well
clear; clc;

temp1 = load('SongData1.mat');
temp2 = load('LabelData1.mat');
temp3 = load('SongFft1.mat');
x1 = temp1.Data;

%h = [1 1 1 1 1 1 1 1 1];
h = zeros(80001,1);
h(1:2000) = 1;
x = zeros(80001,size(x1,2));
for i = 1:size(x1,2)
    x(:,i) = h.*abs(fft(x1(:,i),80001));
end


%% 
clear; clc;

temp1 = load('SongData1.mat');
temp2 = load('LabelData1.mat');
temp3 = load('SongFft1.mat');
x1 = temp1.Data;


i1 = 1;

for lpfvar = 50:10:150
%h = [1 1 1 1 1 1 1 1 1];
h = zeros(80001,1);
%h(1:1000) = 1;
h(1:lpfvar) = 1;
x = zeros(80001,size(x1,2));
for i = 1:size(x1,2)
    x(:,i) = h.*abs(fft(x1(:,i),80001));
end

%

t = temp2.Label;

%i1 = 1;

for hiddenLayerSize = 20
% Choose a Training Function
% For a list of all training functions type: help nntrain
% 'trainlm' is usually fastest.
% 'trainbr' takes longer but may be better for challenging problems.
% 'trainscg' uses less memory. Suitable in low memory situations.

trainFcn = 'trainscg';  % Scaled conjugate gradient backpropagation.

fprintf('Here1\n');
% Create a Fitting Network
% if ( i == 1)
%     hiddenLayerSize = 10;
% else
%     hiddenLayerSize = 20;
% end

    net = fitnet(hiddenLayerSize,trainFcn);

% Choose Input and Output Pre/Post-Processing Functions
% For a list of all processing functions type: help nnprocess
%net.input.processFcns = {'removeconstantrows','mapminmax'};
%net.output.processFcns = {'removeconstantrows','mapminmax'};
fprintf('Here2\n');

% Setup Division of Data for Training, Validation, Testing
% For a list of all data division functions type: help nndivide
net.divideFcn = 'dividerand';  % Divide data randomly
net.divideMode = 'sample';  % Divide up every sample
net.divideParam.trainRatio = 70/100;
net.divideParam.valRatio = 10/100;
net.divideParam.testRatio = 20/100;
fprintf('Here3\n');

% Choose a Performance Function
% For a list of all performance functions type: help nnperformance
net.performFcn = 'mse';  % Mean Squared Error
fprintf('Here4\n');

% Choose Plot Functions
% For a list of all plot functions type: help nnplot
net.plotFcns = {'plotperform','plottrainstate','ploterrhist', ...
    'plotregression', 'plotfit'};
fprintf('Here5\n');

% Train the Network
[net,tr] = train(net,x,t);
fprintf('Here6\n');

% Test the Network
y = net(x);
e = gsubtract(t,y);
performance = perform(net,t,y);
fprintf('Here7\n');

% Recalculate Training, Validation and Test Performance
trainTargets = t .* tr.trainMask{1};
valTargets = t .* tr.valMask{1};
testTargets = t .* tr.testMask{1};
trainPerformance = perform(net,trainTargets,y);
valPerformance = perform(net,valTargets,y);
testPerformance = perform(net,testTargets,y);

% View the Network
%view(net);

% Plots
% Uncomment these lines to enable various plots.
%figure, plotperform(tr)
%figure, plottrainstate(tr)
%figure, ploterrhist(e)
%figure, plotregression(t,y)
%figure, plotfit(net,x,t)

% Deployment
% Change the (false) values to (true) to enable the following code blocks.
% See the help for each generation function for more information.
if (false)
    % Generate MATLAB function for neural network for application
    % deployment in MATLAB scripts or with MATLAB Compiler and Builder
    % tools, or simply to examine the calculations your trained neural
    % network performs.
    genFunction(net,'myNeuralNetworkFunction');
    y = myNeuralNetworkFunction(x);
end
if (false)
    % Generate a matrix-only MATLAB function for neural network code
    % generation with MATLAB Coder tools.
    genFunction(net,'myNeuralNetworkFunction','MatrixOnly','yes');
    y = myNeuralNetworkFunction(x);
end
if (false)
    % Generate a Simulink diagram for simulation or deployment with.
    % Simulink Coder tools.
    gensim(net);
end
clear net;
clear tr;

ErrBar(i1) = norm(e);
PerMet(i1) = performance;
i1 = i1 + 1;
end

end
%
%nhn = 10:10:100;
nhn = 50:10:150;
figure;
bar(nhn,ErrBar);
title('Performance FFT after LPF');
xlabel('Number of Hidden Neurons');
ylabel('||Est. Tempo - Real Tempo||_2^2');
 

